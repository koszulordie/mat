---
title: "Hands on session 4: MLE and Regression"
author: "Ferran Mui√±os"
date: "11/28/2020"
output:
  BiocStyle::html_document:
      toc: true
      toc_float: true
      number_sections: true
      code_folding: hide
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, out.width = "100%", fig.align='center', 
                      message=FALSE, warning = FALSE)
options(width = 1400)
```


# Requirements

## Packages

- [dslabs](https://cran.r-project.org/web/packages/dslabs/dslabs.pdf)
- [rootSolve](https://cran.r-project.org/web/packages/rootSolve/vignettes/rootSolve.pdf)


# Introduction and Objectives

This session will focus on two common themes in computational biology:

1. Multivariable optimization
2. Regression. Fitting generalized linear models (GLM) in R


# Key concepts


## Random processes and likelihood

- Random processes
- Likelihood formula for iid data generated by some random process
- Connection between optimization and GLM

## Multivariate optimization

- Looking for critical points
- Classification of critical points
- Quadratic approximation

## R

- More examples of graphical tools and plots
- Defining functions
- Computing the gradient and the hessian


# Regression

This is a key concept in scientific data analysis. We can briefly put the 
regression problem in general as looking for a function that "best explains"
a collection of response values as a function of some other feature values.

## Set-up

Suppose you have carried out an experiment leading to some table filled out 
with numerical values. As usual, each row represents a sample or replicate in 
our experiment; and each column represents the distinct measurements (features) 
conducted on each sample.

Let $Y$ be a distinguished feature (response variable) that we want to explain
as a function of another set of features: $X_1, \ldots, X_m$. 

In statistics, carrying out regression means to find the random process
that generated the data in $Y$ using as input the set of features. random
means that is subject to some level of randomness. Consequently, unlike the 
case of functions, we do not expect to get always same outputs from identical
inputs.

In general, this problem is very hard. In practice, to make life simple, 
we will search in a relatively small family of such processes (distributions). 

## Ingredients

We need to clarify a few things before daring to work on any 
regression problem at all. At least, we need to specify:

- The set of distributions (family) we will pick our solution from.
- A criterion to ascertain what a good solution is.

Also we will make a technical assumption, that implies that 
all samples in our dataset have been sampled independently
from the same random process.


### Distribution family

It is common that the distributions are taken from a family governed by
a few parameters:

- Gaussian family: $$f_{\mu, \sigma}(x)=\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2} \left(\frac{x-\mu}{\sigma}\right)^2}$$ 
where $\mu$ is the mean and $\sigma$ is the standard deviation. For more information
check the [Normal distribution Wikipedia article](https://en.wikipedia.org/wiki/Normal_distribution).
- Bernoulli/Binomial family: $$g_p(x) = xp + (1-x)(1-p)$$ where $p$ is the probability of drawing a 1.
- Poisson family: $$h_{\lambda}(n) = \frac{\lambda^n}{n!}e^{-\lambda}$$ where $\lambda$ is the mean.

Then finding the right solution is equivalent to finding the **right parameters**.

### Likelihood: standard version without covariates

For each distribution of choice we can compute something called **likelihood of 
the data given the distribution** or **likelihood** for short.
Given the response data $Y = \{y_1,...,y_n\}$, assuming the $y_i$ have been
produced by the random process encoded by the PMF or PDF $f_{\theta}$ (where $\theta$ 
represents a parameter that govern the distribution) the likelihood 
of the data is given by:

$$\mathcal{L}(\theta) = f_{\theta}(y_1)f_{\theta}(y_2)\ldots f_{\theta}(y_n)$$
We will be looking for the $\hat{\theta}$ that gives the maximum $\mathcal{L}(Y,\theta)$. 
In a more compact notation:
$$\hat{\theta} = \textrm{argmax}_{\theta} \; \mathcal{L}(\theta)$$

### Likelihood: with covariates

But what if we want to make the draw dependent on a set of covariates, the 
covariates we have been talking about before? In the context of generalized 
linear models, the trick is that they are hidden inside the parameters of 
the distribution.

### Example

Suppose our dataset has response vector $Y=(y_1, \ldots, y_n)$ and
a covariate vector $X=(x_1, \ldots, x_n)$. If we assume that $Y$ has been 
generated by distributions of a family governed by a parameter $\theta$, we 
will see $\theta$ as being dependent on $X$ in a linear fashion 
$\theta = \alpha X + \beta$. 

This has an important implication: the specific $\theta$ used to 
draw each $y_i$ (say $\theta_i$) depends on the covariate of this sample 
in a linear fashion: $\theta_i = \alpha x_i + \beta$. Now we want to find
$\hat\alpha$ and $\hat\beta$ such that:

$$ \mathcal{L} (\alpha, \beta) = f_{\theta_1}(y_1) f_{\theta_2}(y_2) \ldots f_{\theta_n}(y_n)$$
$$(\hat\alpha, \hat\beta) = \textrm{argmax}_{(\alpha, \beta)} \; \mathcal{L}(\alpha, \beta) $$

# Linear Regression

To illustrate the many faces of linear regression, we will resort once more to 
the [gapminder dataset](https://cran.r-project.org/web/packages/gapminder/README.html).

We want to understand the statistical association between the GDP per capita 
and life expectancy across different countries. Let's have a look at the year 2007.


```{r}
library(tidyverse)
gapminder <- as.data.frame(gapminder::gapminder)
gapminder_2002 <- gapminder %>% filter(year == 2007)
gapminder_sub <- gapminder_2002[, c("country", "lifeExp", "gdpPercap")]
gapminder_sub$log10_gdpPercap <- log10(gapminder_sub$gdpPercap)
head(gapminder_sub)
```

## Solution 1: orthogonal projection onto C(A)

Define the matrix of coefficients $A$ and the independent $b$ term as:

```{r}
num_rows <- nrow(gapminder_sub)
A <- matrix(
  c(gapminder_sub$log10_gdpPercap, rep(1, num_rows)),
  nrow = num_rows,
  ncol = 2,
  byrow = FALSE
)

b <- as.matrix(gapminder_sub$lifeExp)
```

Notice that the $A$ already incorporates a column of ones that renders the 
intercept after multiplying by the unknown vector of linear coefficients. 

We already know (and understand the geometrical meaning behind it, see [tutorial 
on orthogonal projections](https://koszulordie.github.io/mat/2020/11/04/session07.html))
that the solution $\hat{x}$ can be computed as follows:

$$\hat{x} = (A^tA)^{-1} A^t b.$$

```{r}
library(matlib)
xhat <- inv(t(A) %*% A) %*% t(A) %*% b
xhat
```

## Solution 2: GLM function in R

Probably the most compact way to proceed:

```{r}
glm(lifeExp ~ log10_gdpPercap + 1, family=gaussian, gapminder_sub)
```


## Solution 3: optimization

```{r}
squared_error <- function(x){
  v <- c(x[1], x[2]);
  C <- A%*%v - b;
  t(C) %*% C
}
```


The gradient can be computed exactly (see [Exercises for Session 13-14](https://koszulordie.github.io/mat/exercises/exercises-session-14.pdf)):

```{r}
grad_squared_error <- function(x){
  v <- c(x[1], x[2]);
  2 * t(A %*% v - b) %*% A
}
grad_squared_error(xhat)
squared_error(xhat)
```

We can find the root of the gradient function numerically:

```{r}
library(rootSolve)
res <- multiroot(grad_squared_error, c(5,5), atol=c(1e-6,1e-6), maxiter=1e4)
xhat2 <- res$root
xhat2
res$f.root
```

### Graphical representation

```{r}
xs <- seq(10, 20, by=0.05)
ys <- seq(0, 10, by=0.05)

griddf <- expand.grid(alpha=xs, beta=ys)
griddf$value <- apply(griddf, 1, function(x) {squared_error(x)})
```


```{r}
se_gg <- griddf %>%
  ggplot(aes(alpha, beta, z=value)) +
  geom_contour_filled(bins=100, show.legend=FALSE) + 
  geom_point(aes(x=xhat[1], y=xhat[2]), colour="red") + 
  annotate("text", x=xhat[1], y=xhat[2], hjust = 1.2, vjust=1.2, label = "xhat", size = 5, color = "red") +
  coord_fixed(ratio = 1) + 
  labs(x="alpha", y="beta") + 
  ggtitle("Squared residuals function: colormap plot")

se_gg
```


```{r}
se_gg <- griddf %>%
  ggplot(aes(alpha, beta, z=value)) +
  geom_contour(bins=1000, show.legend=FALSE) + 
  geom_point(aes(x=xhat[1], y=xhat[2]), colour="red") + 
  annotate("text", x=xhat[1], y=xhat[2], hjust = 1.2, vjust=1.2, label = "xhat", size = 5, color = "black") +
  coord_fixed(ratio = 1) +
  labs(x="alpha", y="beta") + 
  ggtitle("Squared residuals function: contour plots")

se_gg
```


```{r}
hessian <- 2 * t(A) %*% A
hessian
eigen <- eigen(hessian)

eigenvectors <- eigen$vectors
eigenvectors

eigenvalues <- eigen$values
eigenvalues
```



```{r}
se_gg <- griddf %>%
  ggplot(aes(alpha, beta, z=value)) +
  geom_contour(bins=200, show.legend=FALSE) + 
  geom_point(aes(x=xhat[1], y=xhat[2]), colour="red") +
  geom_segment(aes(x=xhat[1], y=xhat[2], 
                   xend=xhat[1] + eigenvectors[1,1], yend=xhat[2] + eigenvectors[2,1]),
               arrow = arrow(length = unit(0.2, "cm"))) +
  geom_segment(aes(x=xhat[1], y=xhat[2], 
                   xend=xhat[1] + eigenvectors[1,2], yend=xhat[2] + eigenvectors[2,2]),
               arrow = arrow(length = unit(0.2, "cm"))) + 
  coord_fixed(ratio = 1) + 
  annotate("text", x=xhat[1], y=xhat[2], hjust = 1.2, vjust=2.0, label = "xhat", size = 5, color = "black") +
  labs(x="alpha", y="beta") + 
  ggtitle("Squared residuals function: contour plots")

se_gg
```

# Logistic regression

To illustrate logistic regression we will resort to 
the [Breast Cancer Wisconsin Diagnostic Dataset from UCI Machine
Learning Repository](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)) 
imported with **dslabs**.

```{r}
library(dslabs)
brca <- as.data.frame(dslabs::brca)
brca_sub <- brca[, c("x.radius_mean", "x.texture_mean", "y")]
brca_sub$y <- apply(brca_sub, 1, function(x) {x[3]=="M"})
```

We want to understand the statistical association between malignancy of the tumor
(yes or no) and a set of potential predictors derived from biopsies.


## GLM function in R


```{r}
res <- glm(y ~ x.radius_mean + x.texture_mean, family=binomial, data=brca_sub)
res$coefficients
```
## What's going on inside the blackbox?

In logistic regression we assume that the binary response $y_i$, 
is drawn from a Bernoulli with a probability $p_i$ that depends on the 
covariates.

More specifically, if $A\in\mathbb{R}^{n\times m}$ is 
the matrix that contains the values of the $m$ covariates for the $n$ 
samples, and $A_i$ denotes the $i$-th row, then

$$p_i = \frac{e^{A_i{\bf \beta}}}{1 + e^{A_i{\bf \beta}}}$$

where ${\beta} = (\beta_1,\ldots,\beta_m)$ is the vector of unknown 
parameters of the model. 

There is a helpful alternative way to stress this relationship:

$$\log\left(\frac{p_i}{1 - p_i}\right) = A_i{\beta}$$

What the function **glm** does boilds down to computing the parameter $\hat\beta$
that makes the following likelihood maximum:

$$\mathcal{L}(\beta) = \prod_{i=1}^n \left\{ y_ip_i + (1 - y_i)(1 - p_i)\right\} = \prod_{i=1}^n \left\{ y_i \frac{e^{A_i{\bf \beta}}}{1 + e^{A_i{\bf \beta}}} + (1 - y_i)(1 - \frac{e^{A_i{\bf \beta}}}{1 + e^{A_i{\bf \beta}}})\right\}$$
or its log version

$$\log\mathcal{L}(\beta) = \sum_{i=1}^n \log\left\{ y_ip_i + (1 - y_i)(1 - p_i)\right\} = \sum_{i=1}^n y_i\log p_i + (1 - y_i)\log(1 - p_i).$$

# Poisson regression

To illustrate Poisson regression we will resort to the 
[US Contagious Diseases](http://www.tycho.pitt.edu/) dataset imported with **dslabs**.
This dataset portrays yearly counts for Hepatitis A, Measles, Mumps, Pertussis, 
Polio, Rubella, and Smallpox for US states.


```{r}
library(dslabs)
diseases <- as.data.frame(dslabs::us_contagious_diseases)
diseases <- diseases %>% filter(state == "Arizona") %>% filter(disease == "Measles")
diseases$log10_population <- log10(diseases$population)
```

We would like to analyze the trends of measles cases reported in Arizona through 
time but would also like to understand the influence of the population size.

```{r}
res <- glm(count ~ log10_population, family=poisson, data=diseases)
res$coefficients
```


## What's going on inside the blackbox?

In Poisson regression we assume that the count response variable $n_i$, 
is drawn from a Poisson with mean parameter $\lambda_i$ that depends on the 
covariates. 

More specifically, if $A\in\mathbb{R}^{n\times m}$ is 
the matrix that contains the values of the $m$ covariates for the $n$ 
samples, and $A_i$ denotes the $i$-th row, then

$$\lambda_i = e^{A_i \beta}$$

where ${\beta} = (\beta_1,\ldots,\beta_m)$ is the vector of unknown 
parameters of the model. 

What the function **glm** does boilds down to computing the parameter 
$\hat\beta$ that makes the following likelihood maximum:

$$\mathcal{L}(\beta) = \prod_{i=1}^n \frac{\lambda^{n_i}_i}{n_i!}e^{-\lambda_i}$$
or its log version

$$\log\mathcal{L}(\beta) = \sum_{i=1}^n -\lambda_i + n_i\log\lambda_i - {n_i}! = k + \sum_{i=1}^n -e^{A_i\beta} + n_i A_i\beta$$
where $k$ is a constant terms which does not depend on $\beta$.

```{r}
sessionInfo()
```
