---
title: "Principal Component Analysis in Bioinformatics"
author: "Ferran Mui√±os and Ramon Massoni-Badosa"
date: "11/08/2020"
output: 
  BiocStyle::html_document:
    toc: true
    toc_float: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction and objectives

In this hands-on session, we will show how principal component analysis (PCA) is used in common bioinformatics workflows. We will use a toy [single-cell RNA-seq (scRNA-seq)](https://www.nature.com/articles/s41596-018-0073-y) dataset to illustrate it.

The main objectives are the following:

- Understand the intuition behind PCA.
- Learn to compute PCA in 3 ways in R: by eigendecomposition of the covariance matrix, by singular value decomposition and using the `prcomp` function.
- Perform a case-study using scRNA-seq data.


# Introduction to scRNA-seq

Single-cell RNA-seq allows the gene expression profiling of thousands of cells, one a time. It is often exemplified using the following analogy:

![](../img/single-cell_fruit_salad.png)

* Bulk RNA-seq would be analogous to a fruit smoothie, in which all we get is an average taste out of several fruits. Analogously, in bulk RNA-seq all we get is an average transcriptome across a population of cells.
* scRNA-seq, on the other hand, is analogous to a fruit salad, in which you can taste one fruit at a time. Here, we get a transcriptome for each single cell, so we can understand cell-to-cell variability in gene expression.

The scRNA-seq pipeline can be summarized in these 4 steps:

![](../img/single-cell_workflow.png)
Image extracted from [this paper](https://www.nature.com/articles/s41596-018-0073-y).


1. Sample preparation: cells are dissociated from their original tissue into single-cells using enzymatic or mechanical treatments. In addition, we can use FACS to enrich for a cell type of interest.
2. Single-cell RNA-sequencing, which includes: single-cell capture (into droplets or wells), lysis, mRNA capture, reverse transcription, PCR amplification and Illumina sequencing.
3. Data processing: map the fastq reads to obtain the gene expression matrix (cells by genes).
4. Data analysis, which includes clustering, differential expression analysis, gene regulatory networks, etc.


# Why use PCA?

scRNA-seq is a double-edged sword. On the one hand, scRNA-seq provides unprecedented discriminative power to chart all the cell types and states in a given tissue. This has catalyzed the creation of [the Human Cell Atlas (HCA)](https://elifesciences.org/articles/27041), which aims to classify all cells in the human body using scRNA-seq; and has been coined as "the periodic table of human cells".

![](../img/human_cell_atlas.png)

On the other hand, however, scRNA-seq has a high degree of technical variability, which can be introduced at multiple points of the aforementioned pipeline. Some examples include the following:

* Dissociation artifacts: enzymatic treatment (ie collagenase) is frequently used to dissociate tissues into single-cells. This can induce cellular stress that can be confounded by the variable of interest. Described in detail in [this paper](https://www.nature.com/articles/nmeth.4437?proof=t).
* Doublets: Two cells that are processed together in a reaction volume (e.g., a well or droplet) and receive the same single-cell barcode.
* Dropouts: Transcripts that are not detected in the final dataset even though the gene is expressed in the cell, leading to false zero values in the expression matrix. (Definitions extracted from table 1 of [this paper)](https://www.nature.com/articles/s41596-018-0073-y).


In this scenario, PCA is used for 3 main purposes:

1. Reduce the noise of the dataset: by finding the orthogonal axis that maximize the variance, PCA eliminates noise introduced by dropouts, transcriptional bursts, etc.
2. Reduce the redundancy: as we see below, there is a vast degree of redundancy across gene sets. Thus, principal components can be interpreted as a "metagene": a score that summarizes information from a correlated set of genes.
3. Reduce computational complexity: since we reduce the dimensionality, operations downstream such as clustering will be executed faster.


# Case-study

As an example, we will create an expression matrix with the following cells:

* 4 T-cells (identified by high expression of CD3D and CD3E).
* 3 monocytes (identified by high expression of LYZ and S100A8).
* 3 naive B-cells (identified by high expression of CD79A, CD79B and BLNK).
* 2 plasma cells (identified by B-cell and proliferation markers, such as TOP2A or MKI67).
* 2 poor-quality cells (identified by high mitochondrial expression). If a cell has pores in the membrane due to cell lysis, the cytosolic mRNA will leak out of the cell; but if the diameter of mitochondria is higher than the pores, they will get trapped inside the cell.


## Load packages

```{r}
library(pheatmap)
library(tidyverse)
```

## Create and visualize toy dataset

```{r}
# Create toy dataset
toy <- data.frame(
  CD3D = c(4, 5, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
  CD3E = c(5, 3, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
  LYZ = c(0, 0, 0, 0, 5, 7, 6, 0, 0, 0, 0, 0, 0, 0),
  S100A8 = c(0, 0, 0, 0, 5, 7, 6, 0, 0, 0, 0, 0, 0, 0),
  CD79A = c(0, 0, 0, 0, 0, 0, 0, 6, 4, 4, 3, 5, 0, 0),
  CD79B = c(0, 0, 0, 0, 0, 0, 0, 5, 5, 3, 4, 6, 0, 0),
  BLNK = c(0, 0, 0, 0, 0, 0, 0, 6, 4, 5, 5, 4, 0, 0),
  TOP2A = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 9, 0, 0),
  MKI67 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 10, 0, 0),
  MT_CO1 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 18, 23),
  MT_ND5 = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 22, 25) 
)
rownames(toy) <- paste("cell", as.character(1:nrow(toy)))
toy <- as.matrix(toy)


# Visualize
pheatmap(toy, cluster_cols = FALSE, cluster_rows = FALSE, angle_col = 45)
```
In the following image you can visualize the redundancy between CD79A and CD79B. Together, they form the heterodimer CD79; which means that for every molecule of CD79A we need one of CD79B. Under the hood they are the same variable, which should be captured by a single principal component. A good analogy would be to measure the height of a person in feet and in cm.

![](../img/CD79_figureA.png)
![](../img/CD79_figureB.png)

Images obtained from [this link](https://ashpublications.org/blood/article/120/6/1175/30492/The-B-cell-receptor-signaling-pathway-as-a)


## PCA in R: 3 ways

Three ways to perform PCA in R: 

* Eigendecomposition of the covariance matrix.
* Singular Value Decomposition.
* `prcomp` function.

Regardless of the method, we always need to center the data:

```{r}
toy_centered <- scale(toy, center = TRUE, scale = FALSE)
toy_centered[1:6,1:6]
```

Now we will compute the exact same thing (PCA) in the 3 ways. 

In each we will be tracking the same three pieces of information:

* the principal directions (pdir), also known as loadings.
* the variance of the data along each principal direction (var).
* the scores of each data sample in the principal directions (score).

### Eigendecompostion of the covariance matrix

We will be computing an eigenbasis for the covariance matrix 
$\Omega = \frac{1}{n-1}A^tA$ where $A$ is the centered data.

```{r}
cov_matrix <- (1 / (nrow(toy_centered) - 1)) * (t(toy_centered) %*% toy_centered)
eigen_cov_matrix <- eigen(cov_matrix)

pdir_eigen <- eigen_cov_matrix$vectors
pdir_eigen[1:6,1:6]

var_eigen <- eigen_cov_matrix$values
head(var_eigen)

scores_eigen <- toy_centered %*% pdir_eigen
scores_eigen[1:6, 1:6]
```

### SVD

We can apply SVD to the centered data $A$. Observe that if we want the singular 
values to reflect the same statistical information as above, we need to correct
by a factor of $1/\sqrt{n-1}$ where $n$ is the number of rows of $A$.

```{r}
toy_to_svd <- toy_centered
toy_svd <- svd(toy_to_svd, nu = nrow(toy_to_svd))

pdir_svd <- toy_svd$v
pdir_svd[1:6,1:6]

d_adjusted <- toy_svd$d * (1 / sqrt(nrow(toy) - 1))
var_svd <- d_adjusted ** 2
head(var_svd)

sigma <- diag(toy_svd$d, nrow(toy_svd$u), ncol(toy_svd$v))
scores_svd <- toy_svd$u %*% sigma
scores_svd[1:6,1:6]
```

### PCA

Stand-alone PCA function provided by R:

```{r}
pca_out <- prcomp(toy, center = TRUE)
```

Use the following keywords to parse the output instance:

* principal directions: "rotation"
* standard deviations a.k.a. singular values: "sdev"
* PC scores: "x"

```{r}
pdir_pca <- pca_out$rotation
pdir_pca[1:6,1:6]

var_pca <- pca_out$sdev ** 2
head(var_pca)

scores_pca <- pca_out$x
scores_pca[1:6,1:6]
```

```{r}
# PCA centered and scaled
pca_out_scaled <- prcomp(toy, center = TRUE, scale. = TRUE)
```

Importantly, we obtain the following:

* Gene loadings: `pca_out$rotation`
* PC scores: `pca_out$x`
* Variance associated to each PC: `pca_out$sdev ** 2`


In addition, we can get the proportion of variance explained (PVE) and the cumulative proportion with the `summary` function (more info in [this book](https://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf)):

```{r}
summary(pca_out_scaled)
```

## Infer dimensionality of the dataset

To reduce the dimensionality, we will choose the number of PC that explains most of the variance in the dataset. To this end, we use a so-called elbow plot:


```{r}
# Calculate percentage of variance explained (PVE):
pve <- pca_out_scaled$sdev ** 2 / sum(pca_out$sdev ** 2) * 100
pve <- round(pve, 2)
pve_df <- data.frame(principal_component = 1:length(pve), pve = pve)
pve_gg <- pve_df %>%
  ggplot(aes(principal_component, pve)) +
    geom_point() +
    geom_vline(xintercept = 4.5, linetype = "dashed", color = "darkblue") +
    scale_x_continuous(breaks = 1:length(pve)) +
    labs(x = "Principal Component", y = "Percentage of Variance Explained (%)") +
    theme_bw()
pve_gg
```
Some people prefer to plot the cumulative percentage of explained variance:

```{r}
summ_pca <- summary(pca_out_scaled)
cum_pct <- summ_pca$importance["Cumulative Proportion", ] * 100
cum_pct_df <- data.frame(principal_component = 1:length(pve), cum_pct = cum_pct)
cum_pct_gg <- cum_pct_df %>%
  ggplot(aes(principal_component, cum_pct)) +
    geom_point() +
    geom_vline(xintercept = 4.5, linetype = "dashed", color = "darkblue") +
    scale_x_continuous(breaks = 1:length(cum_pct)) +
    scale_y_continuous(limits = c(0, 100)) +
    labs(x = "Principal Component", y = "Cumulative Percentage of Variance Explained (%)") +
    theme_bw()
cum_pct_gg
```


Thus, we conclude that the first 4 PC are significant. Let us express each cell as a vector of the first 4 PC:

```{r}
toy_reduced <- pca_out_scaled$x[, c("PC1", "PC2", "PC3", "PC4")]
gene_loads_reduced <- pca_out_scaled$rotation[, c("PC1", "PC2", "PC3", "PC4")]
pheatmap(toy_reduced, cluster_rows = FALSE, cluster_cols = FALSE, angle_col = 45)
```


## Visualize gene loadings

Now we have reduced the dimensionality of the dataset. To interpret each PC, let us inspect the gene loadings:

```{r}
significant_pcs <- c("PC1", "PC2", "PC3", "PC4")
loadings_gg <- map(significant_pcs, function(x) {
  loadings <- gene_loads_reduced[, x]
  df <- data.frame(gene = names(loadings), score = loadings)
  p <- df %>%
    ggplot(aes(fct_reorder(gene, score), score)) +
      geom_point() +
      labs(x = "", y = x) +
      theme_bw() +
      coord_flip()
  return(p)
})
loadings_gg
```

Interpretation of the PC:

* PC1: B cell identity.
* PC2: differences between monocytes and T cells.
* PC3: technical variation.
* PC4: cell cycle.


## Cluster cells

Clustering means classifying observations into groups that minimize and maximize intra-group and inter-group distances, respectively.


### Calculate all pairwise Euclidean distances

To that end we need a concept of distance, which basically measures how similar or different two vectors are. In our case we will use Euclidean distance, but be aware that there are a plethora of different options that can yield different clustering results.

Let us start by computing all pairwise distances:

```{r}
dist_mat <- dist(toy_reduced, method = "euclidean")
pheatmap(dist_mat, cluster_rows = FALSE, cluster_cols = FALSE)
```


## Perform hierarchical clustering

```{r}
hclust_average <- hclust(dist_mat, method = "average")
plot(
  hclust_average,
  labels = rownames(toy),
  main = "Average Linkage",
  xlab = "",
  sub = "",
  ylab = ""
)
```


### Cut the dendrogram and visualize clusters

```{r}
plot(
  hclust_average,
  labels = rownames(toy),
  main = "Average Linkage",
  xlab = "",
  sub = "",
  ylab = ""
)
abline(h = 2.5, col = "red")
hc_clusters <- cutree(hclust_average, h = 2.5)
hc_clusters
table(hc_clusters)
annotation_rows <- data.frame(cluster = as.character(hc_clusters))
rownames(annotation_rows) <- names(hc_clusters)
pheatmap(
  toy_centered,
  cluster_rows = FALSE,
  cluster_cols = FALSE,
  angle_col = 315,
  annotation_row = annotation_rows
)
```

### Annotation

| Cluster | Cell type          |
|---------|--------------------|
| 1       | T-cells            |
| 2       | Monocytes          |
| 3       | Naive B-cells      |
| 4       | Plasma Cells       |
| 5       | poor-quality cells |


```{r}
annotated_clusters <- c("T-cells", "Monocytes", "Naive B-cells", "Plasma Cells", "poor-quality cells")
names(annotated_clusters) <- as.character(1:5)
annotation_rows$annotation <- annotated_clusters[annotation_rows$cluster]
pheatmap(
  toy_centered,
  cluster_rows = FALSE,
  cluster_cols = FALSE,
  angle_col = 315,
  annotation_row = annotation_rows
)
```



# Session Information

```{r}
sessionInfo()
```

