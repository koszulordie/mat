\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}


%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\rank}{\textrm{rank}}
\newcommand{\length}{\textrm{length}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\C}{\bb{C}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}
\newcommand\yellow[1]{\colorbox{yellow}{#1}}
\newcommand\ans{\underline{Answer}: }

%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
\textbf{Final Test (Monday 13th Dec 2021): Solutions}\\
Elements of Mathematics -- Bioinformatics for Health Sciences \\
\end{center}

\vspace{0.2 cm}

\begin{enumerate}


% ----1----

\item {\bf (2 points)} Suppose that $A$ is a square matrix with $\det(A)\neq0$.
\begin{itemize}
\item[a)] {\bf (1 point)} Explain why $\rank(A^2) = \rank(A)$.
\item[b)] {\bf (1 point)} Find an example of a $2\times 2$ matrix such that $\rank(A^2) < \rank(A)$. 
\end{itemize}
\ans 
\begin{itemize}
\item[a)] Both $A$ and $A^2$ must be full rank square matrices since $\det(A)\neq 0$ and $\det(A^2) = \det(A)^2 \neq 0$.
\item[b)] By virtue of the first point such an example must satisfy $\rank(A) = 1$, so we need to find a rank 1 matrix $A$ such that $\rank(A^2)=0$, in other words, $A^2$ is the zero matrix. Alternatively, one can search for a rank 1 matrix $A$ such that $C(A) \subset N(A)$. Note that the matrices
\[
\begin{bmatrix}
0 & 0 \\
1 & 0 
\end{bmatrix}
\;\;\;
\begin{bmatrix}
0 & 1 \\
0 & 0 
\end{bmatrix}
\]
would both work.

\end{itemize}

% ----2----

\item {\bf (1.5 points)} Consider the following matrix:

\[
A = \begin{bmatrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1
\end{bmatrix}
\]
\begin{itemize}
\item[a)] {\bf (0.5 point)} Find a basis of $C(A)$ the column space of $A$.
\item[b)] {\bf (0.5 point)} Find a basis of $N(A)$ the null space of $A$.
\item[c)] {\bf (0.5 point)} Find an orthonormal basis of $N(A)$.
\end{itemize}

\ans
\begin{itemize}
\item[a)] $\mathcal{C}=\{(1,1,1)\}$ this is immediate. 
\item[b)] Note that $\dim C(A) = 3 - \dim N(A)$ which enforces $\dim N(A)=2$. The vectors $(1,0,-1), (0,1,-1)$ are easy to find members of $N(A)$ and are linearly independent, so they must also be a generating set, then $\mathcal{N}=\{(1,0,-1), (0,1,-1) \}$ is a basis of $N(A)$.
\item[c)] Let $u=(1,0,-1)$ and $v=(0,1,-1)$. Note that $\mathcal{N} = \{u,v\}$ is not an orthonormal basis of $N(A)$ because $u\cdot v=1\neq 0$. One way to go is to substract from $v$ its orthogonal projection onto $\textrm{span}(u)$. Let's compute the matrix of this orthogonal projection. Taking $\tilde{u} = u / \length(u)$, we can build the sought orthogonal projection as:
\[
P = \tilde{u}\tilde{u}^t = 
\frac{1}{\sqrt{2}}\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 0 & -1 
\end{bmatrix}
=
\frac{1}{2}\begin{bmatrix}
1 & 0 & -1 \\
0 & 0 & 0 \\
-1 & 0 & 1
\end{bmatrix}
\]
The non-zero vector $e = v - Pv$ is orthogonal to $u$:
\[
e = v - Pv =  
\begin{bmatrix}
0 \\
1 \\
-1
\end{bmatrix}
-\frac{1}{2}\begin{bmatrix}
1 & 0 & -1 \\
0 & 0 & 0 \\
-1 & 0 & 1
\end{bmatrix}
\begin{bmatrix}
0 \\
1 \\
-1
\end{bmatrix} = 
\begin{bmatrix}
0 \\
1 \\
-1
\end{bmatrix}-
\begin{bmatrix}
1/2 \\
0 \\
-1/2
\end{bmatrix}=
\begin{bmatrix}
-1/2 \\
1 \\
-1/2
\end{bmatrix}
\]
\end{itemize}

Therefore taking $\tilde{e} = e/\length(e) = (-1/\sqrt{6},2/\sqrt{6},-1/\sqrt{6})$ we get 
$\mathcal{O}=\{\tilde{u}, \tilde{e}\}$ an orthonormal basis of $N(A)$.

% ----3----

\item {\bf (1.5 points)} Consider a matrix $A\in\mathbb{R}^{n\times 3}$ with colum vectors $u, v, w \in\mathbb{R}^n$, i.e., $A=[\, u \,|\, v \,|\, w \,]$.

\begin{itemize}
\item[a)] {\bf (0.5 point)} Find a matrix $E\in\mathbb{R}^{3\times 3}$ such that $AE=[\,u \,|\, v \,|\, w-u-2v\,]$.
\item[b)] {\bf (0.5 point)} Find a matrix $P\in\mathbb{R}^{3\times 3}$ such that $AP = [\,w \,|\, u \,|\, v\,]$.
\item[c)] {\bf (0.5 point)} Find $P^{-1}$ the inverse matrix of $P$.
\end{itemize}

\ans

\begin{itemize}
\item[a)] 
\[
E = \begin{bmatrix}
1 & 0 & -1 \\
0 & 1 & -2 \\
0 & 0 & 1
\end{bmatrix}
\]

\item[b)] 

\[
P = \begin{bmatrix}
0 & 1 & 0 \\
0 & 0 & 1 \\
1 & 0 & 0
\end{bmatrix}
\]

\item[c)] $P$ is an orthogonal matrix, so its inverse is its transpose. Alternatively, we may observe that $P^{-1}$ should restore the original arrangement of columns so that $APP^{-1} = [\,u \,|\, v \,|\, w\,]$, which can be accomplished if $AP^{-1}=[\, v \,|\, w \,|\, u\,]$. In any case,

\[
P^{-1} = \begin{bmatrix}
0 & 0 & 1 \\
1 & 0 & 0 \\
0 & 1 & 0
\end{bmatrix}
\]


\end{itemize}


% ----4----

\item {\bf (1.5 points)}  Consider the following matrix:

\[
A = 
\begin{bmatrix}
1 & 1 \\
1 & -1 \\
1 & 1 
\end{bmatrix}
\]

\begin{itemize}
\item[a)] {\bf (0.5 point)} Find the eigenvalues of $\Omega=A^tA$.
\item[b)] {\bf (0.5 point)} Find a basis of $\mathbb{R}^2$ of eigenvectors of $\Omega$.
\item[c)] {\bf (0.5 point)} Compute $\Omega^5$.
\end{itemize}

\ans
\begin{itemize}
\item[a)] 
\[
\Omega = A^tA = 
\begin{bmatrix}
3 & 1 \\
1 & 3
\end{bmatrix}
\]
The eigenvalues of $\Omega$ must satisfy $\lambda_1 + \lambda_2 = \textrm{tr}(\Omega) = 3 + 3 = 6$ 
and $\lambda_1\lambda_2 = \det(\Omega) = 9 - 1 = 8$, thus $\lambda_1=2$ and $\lambda_2=4$ work.

\item[b)]
\[
\Omega - \lambda_1I =  
\begin{bmatrix}
1 & 1 \\
1 & 1
\end{bmatrix}
\] whence $N(\Omega - \lambda_1I) = \textrm{span}\{(1,-1)\}$

\[
\Omega - \lambda_2I =  
\begin{bmatrix}
-1 & 1 \\
1 & -1
\end{bmatrix}
\] whence $N(\Omega - \lambda_2I) = \textrm{span}\{(1,1)\}$. 

Then $\mathcal{B} = \{(1,-1), (1,1)\}$ is a basis of $\mathbb{R}^2$ of eigenvectors of $\Omega$. 


\item[c)]

We can  factor $\Omega$ using the ``decoder'' matrix $C$, the ``encoder'' matrix $C^{-1}$ and the diagonal matrix $\Lambda=\textrm{diag(2, 4)}$ as $\Omega = C \Lambda C^{-1}$. Then $\Omega^5 = C \Lambda^5 C^{-1}$.

\[
C =  
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\]

\[
C^{-1} = \frac{1}{2}\begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}
\]

\[
\Omega^5 = C \Lambda^5 C^{-1} = \frac{1}{2}
\begin{bmatrix}
1 & 1 \\
-1 & 1
\end{bmatrix}
\begin{bmatrix}
2^5 & 0 \\
0 & 4^5
\end{bmatrix}
\begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}=\frac{1}{2}
\begin{bmatrix}
2^5 & 4^5 \\
-2^5 & 4^5
\end{bmatrix}
\begin{bmatrix}
1 & -1 \\
1 & 1
\end{bmatrix}=\frac{1}{2}
\begin{bmatrix}
4^5 + 2^5 & 4^5 - 2^5 \\
4^5 - 2^5 & 4^5 + 2^5
\end{bmatrix}
\]


\end{itemize}



% ----5----

\item {\bf (1.5 point)} Consider the following function:

\[
f(x,y) = \frac{1}{1 + e^{-ax-by}}
\]

which results from composing the (single-variable) standard logistic function

\[
\sigma(x) = \frac{1}{1 + e^{-x}}
\]

and the two-variable linear function.

\[
L(x, y) = ax+by,
\]

where $a,b\in\mathbb{R}$ are parameters of the function.

\begin{itemize}
\item[a)] {\bf (0.5 point)} Verify that $\sigma'(x) = \sigma(x)(1-\sigma(x))$
\item[b)] {\bf (1 point)} Compute $\nabla f (0,0)$, i.e., the gradient vector of $f$ at the point (0,0).
\end{itemize}

\ans
\begin{itemize}
\item[a)] On the one hand, by using the chain rule we can compute the derivative of the logistic function:
\[
\sigma'(x) = \frac{-1}{(1 + e^{-x})^2} \cdot (-e^{-x}) = \frac{e^{-x}}{(1-e^{-x})^2}
\]
On the other hand, we can develop the expression $\sigma(x)(1-\sigma(x))$:
\[
\sigma(x)(1-\sigma(x)) = \frac{1}{1 + e^{-x}} \cdot \frac{e^{-x}}{1 + e^{-x}} = \frac{e^{-x}}{(1-e^{-x})^2}
\]

\item[b)]

\[
\frac{\partial f}{\partial x} =  \frac{-1}{(1 + e^{-ax-by})^2} \cdot (-ae^{-xa-by}) = \frac{ae^{-xa-by}}{(1 + e^{-ax-by})^2}
\]
\[
\frac{\partial f}{\partial y} =  \frac{-1}{(1 + e^{-ax-by})^2} \cdot (-be^{-xa-by}) = \frac{be^{-xa-by}}{(1 + e^{-ax-by})^2}
\]

Since the gradient is $\nabla f(x,y) = (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y})$, $\nabla f(0,0) =(a/4,b/4)$.
\end{itemize}


% ----6----

\item {\bf (2 points)} Determine the nature of the critical point $(0,0)$ of the fuction
\[
f(x, y) = x^3 + 2xy^2 + x^2  + 3xy + y^2 + 1.
\]

\ans

Let's compute the partial derivatives, then the eigenvalues of the Hessian matrix of $f$ at $(0,0)$:
\begin{align*}
\frac{\partial f}{\partial x} &= 3x^2 + 2y^2 + 2x+ 3y & \frac{\partial f}{\partial y} &= 4xy + 3x + 2y \\
\frac{\partial^2 f}{\partial x^2} &= 6x + 2 & \frac{\partial^2 f}{\partial x \partial y} &= 4y + 3 & \frac{\partial^2 f}{\partial y^2} &= 4x + 2
\end{align*}
Then the Hessian of $f$ at $(0,0)$ is the matrix:
\[
H = Hf(0,0) = \begin{bmatrix}
2 & 3 \\
3 & 2
\end{bmatrix}.
\]

The eigenvalues of $H$ must satisfy $\lambda_1 + \lambda_2 = \textrm{tr}(H) = 2 + 2 = 4$ 
and $\lambda_1\lambda_2 = \det(H) = 4 - 9 = -5$, thus $\lambda_1=-1$ and $\lambda_2=5$ work. 
Since $\lambda_1\lambda_2 < 0$, $(0,0)$ is neither a local maximum nor a local minimum, but a saddle point.
\end{enumerate}

\end{document}


